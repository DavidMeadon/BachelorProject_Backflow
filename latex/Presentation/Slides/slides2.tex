% \begin{frame}{This is where you start}
%     Add your frames below this and then I'll sort out the format later
% \end{frame}

%\AtBeginSection[]{% Print an outline at the beginning of sections
    \begin{frame}%<beamer>
      \frametitle{Outline}% for Section \thesection}
      \tableofcontents%[currentsection]
    \end{frame}%}
    
    \section{Introduction}
    \subsection{Motivating Problem}
    \begin{frame}{Motivating Problem}
      \includegraphics[scale=0.4]{resources/Molly.PNG}
    \end{frame}
    \subsection{Problem Description}
    \begin{frame}{Main Question}
    \begin{center}
        \textbf{"To What Extent Should Content in Social Media be Filtered?"}
    \end{center}
    \begin{block}{Sub-questions}
        \begin{enumerate}
       \item What content should be filtered?
       \item Who should carry out the regulation?
       \item How effective are filters?
       \end{enumerate}
       \end{block}
    \end{frame}
    
    \section{Main Part} %%   CHANGE THIS NAME
    \subsection{What content should be filtered?}
    \begin{frame} {What content should be filtered?}
 
        \begin{block}{Why is this sub-question relevant?}
        \begin{itemize}
            \item To narrow down the scope
        \end{itemize}
        \end{block}
        \begin{block} {To be examined:}
        \begin{itemize}
          \item Copy-righted content 
          \item Terrorism content    
          \item Self-harm content   

           \end{itemize}
        \end{block}
    \end{frame}
  
    \subsection{Who should carry out the regulation?}
    \begin{frame}{Who should carry out the regulation?}

        \begin{block}  {Why is this sub-question relevant?}
        \begin{itemize}
            \item To determine the level of conflicts (micro,meso,macro)
        \end{itemize}
        \end{block}
        
        \begin{block} {What parties are involved} 
        \begin{itemize}
            \item Users
            \item Social Media Providers (Facebook, Instagram, etc)
            \item Governments
            %\item Academics
            % Can refer: The next subquestion we will examine is~. THis subquestion is toward to the main question because we can determine the level of conflicts for example, individual, social media providers and governments. we can ask question If harmful contents or hate speech on social media caused physical harm who should get responsibility. Individual, social media providers, goverments. the reason why they have responsibility and what is downside if the duty performed only by individual and provider.-> we need law-> government need to engage in regulating at some lever.
        \end{itemize}
        \end{block}

    \end{frame}
    \subsection{How effective are filters?}
    \begin{frame}{How effective are filters?}
    \begin{block}{Why is this sub-question relevant?}
        \begin{itemize}
                \item How Viable are they to Implement
                \item Their Actual Effectiveness in Blocking Specific Content %% both kind of similar
            \end{itemize}
      \end{block}
      
      \begin{block}{What we will consider}
        \begin{itemize}
                \item Comparison of Different Filtering Techniques (DNS Poisoning, IP Packet Filtering and URL Blocking) 
                \item Over-Blocking vs. Under-Blocking
                \item Circumventing Filtering
            \end{itemize}
      \end{block}
    \end{frame}
    

 
    
    \section{Conclusion}
    \subsection{Conclusion}
    \begin{frame}{Conclusion}
    %first sub:from the view point of consequentilist, we shouldn't enhance regulation for copyrighted contents but should filter extremist contents. however deontological view shows that we should filter both. second sub:They all have responsibilities according to  deontology, however we can't avoid regulating conducted by government.  so the our answer to main question is~
    \begin{block}{Content to always be filtered}
    \begin{itemize}
        \item Explicit content regarding death or injury
        \item Terrorist Activity
        \item More generally content that directly harms people's lives
    \end{itemize}
    \end{block}
    
    All other content should only be blocked if the algorithm can filter it with enough accuracy.
        
    \end{frame}
    \begin{frame}[label=bibliography]{Bibliography}
    \footnotesize
      %\framesubtitle{\TeX, \LaTeX, and Beamer}
      \begin{thebibliography}{9}
        \bibitem{knuth84}
             T.~Hughes.
            \emph{“Instagram helps girls self-harm”} In Daily Star.
          Sep. 2018. [Online]. Available at \url{https://www.dailystar.co.uk/real-life/728459/Instagram-helps-girls-self-harm.}
        \bibitem{lamport94}
              T. Heyman, 
              \emph{“EU plans new laws to target terror on social media sites”},The  National,Aug.  2018.  [Online].  Available at \url{https://www.thenational.ae/world/europe/eu-plans-new-laws-to-target-terror-on-social-media-sites-1.762013}
        \bibitem{MG94}
            H. T.Tavani,
            \emph{Ethics and Technology: Controversies, Questions, and Strategies for Ethical Computing, fourth.}
            Wiley, 2012,isbn: 9781118281727.
        \bibitem{tantau04}
            C.  D.  Hunter, 
            \emph{“Internet  filter  effectiveness  -  testing  over-  and  underinclusive  blockingdecisions  of  four  popular  web  filters”}
            ,Social  Science  Computer  Review,  vol.  18,  no.  2,pp. 214–222, 2000. doi: \url{https://doi.org/10.1177/089443930001800209}
        %\bibitem{MS05}
         %    LexisNexisR©,
          %   \emph{Survey  of  Law  Enforcement  Personnel  and  Their  Use  of  Social  Media,}
           %  LexisNexisR©Risk Solutions, 2014. [Online]. Available at \url{www.lexisnexis.com/investigations.}
      \end{thebibliography}
    \end{frame}
    
    \begin{frame}{}
        \begin{center}
            \huge \textbf{Questions?}
        \end{center}
    \end{frame}
    
